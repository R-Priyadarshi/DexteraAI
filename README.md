# DexteraAI â€” Gesture Intelligence Platform

> Real-time, on-device, privacy-preserving hand-gesture recognition platform.
> Web Â· Mobile Â· Desktop Â· Embedded Â· Robotics

[![CI](https://github.com/R-Priyadarshi/DexteraAI/actions/workflows/ci.yml/badge.svg)](https://github.com/R-Priyadarshi/DexteraAI/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

---

## ğŸ¯ What This Is

DexteraAI is a **gesture intelligence platform** â€” not a demo. It provides:

- **Real-time hand landmark detection** via MediaPipe
- **Transformer-based temporal gesture modeling** for sequence recognition
- **On-device inference** (zero cloud, zero data leakage)
- **Cross-platform**: Web (WebGPU), Mobile (TFLite), Desktop (ONNX), Edge, Robotics
- **Few-shot & zero-shot gesture learning**
- **Accessibility-first** design (motor disability support)

## ğŸ— Project Structure

```
DexteraAI/
â”œâ”€â”€ core/                  # Core ML pipeline (platform-agnostic)
â”‚   â”œâ”€â”€ types.py           # Shared types, protocols, constants
â”‚   â”œâ”€â”€ vision/            # MediaPipe hand detection + preprocessing
â”‚   â”‚   â”œâ”€â”€ detector.py
â”‚   â”‚   â””â”€â”€ preprocessor.py
â”‚   â”œâ”€â”€ landmarks/         # Landmark normalization, augmentation, features
â”‚   â”‚   â”œâ”€â”€ normalizer.py
â”‚   â”‚   â”œâ”€â”€ augmentor.py
â”‚   â”‚   â””â”€â”€ features.py
â”‚   â”œâ”€â”€ temporal/          # Transformer gesture sequence model
â”‚   â”‚   â”œâ”€â”€ model.py       # GestureTransformer (temporal, CLS token)
â”‚   â”‚   â”œâ”€â”€ static_model.py# StaticGestureClassifier (single-frame MLP)
â”‚   â”‚   â””â”€â”€ sequence_buffer.py
â”‚   â”œâ”€â”€ inference/         # ONNX runtime + full pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ onnx_runtime.py
â”‚   â”‚   â””â”€â”€ pipeline.py
â”‚   â””â”€â”€ calibration/       # Per-user gesture calibration
â”‚       â””â”€â”€ calibrator.py
â”œâ”€â”€ training/              # Training pipeline
â”‚   â”œâ”€â”€ datasets/          # Dataset loaders (real + synthetic)
â”‚   â”‚   â””â”€â”€ gesture_dataset.py
â”‚   â”œâ”€â”€ trainers/          # PyTorch training loop (AMP, MLflow, early stopping)
â”‚   â”‚   â””â”€â”€ train_gesture.py
â”‚   â”œâ”€â”€ evaluation/        # Metrics, confusion matrix, latency benchmarks
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ export/            # ONNX / TFLite export + quantization
â”‚       â”œâ”€â”€ to_onnx.py
â”‚       â””â”€â”€ to_tflite.py
â”œâ”€â”€ backend/               # FastAPI server (OPTIONAL â€” for remote inference)
â”‚   â”œâ”€â”€ config.py          # Pydantic settings
â”‚   â”œâ”€â”€ logging_config.py  # Loguru structured logging
â”‚   â””â”€â”€ apps/api/          # REST + WebSocket endpoints
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ routes.py
â”‚       â”œâ”€â”€ schemas.py
â”‚       â”œâ”€â”€ middleware.py
â”‚       â””â”€â”€ dependencies.py
â”œâ”€â”€ apps/                  # Application layer
â”‚   â”œâ”€â”€ web/               # Next.js + ONNX Runtime Web + WebGPU
â”‚   â”œâ”€â”€ desktop/           # Tauri (Rust + Web frontend)
â”‚   â””â”€â”€ mobile/            # Flutter (planned)
â”œâ”€â”€ tests/                 # pytest + hypothesis
â”œâ”€â”€ docs/                  # Architecture docs, model cards, API reference
â”œâ”€â”€ dextera.py             # CLI: train / eval / export / demo / benchmark / serve / info
â”œâ”€â”€ pyproject.toml         # Python project config + all dependencies
â”œâ”€â”€ Makefile               # Developer convenience commands
â”œâ”€â”€ Dockerfile             # Multi-stage production container
â”œâ”€â”€ dvc.yaml               # DVC pipeline (data â†’ train â†’ eval â†’ export)
â””â”€â”€ .github/workflows/     # CI/CD (lint, test, benchmark)
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Node.js 20+ (web app only)

### Install & Test
```bash
git clone https://github.com/R-Priyadarshi/DexteraAI.git
cd DexteraAI

# Install all dependencies
make dev
# â€” or manually â€”
pip install -e ".[dev,training]"

# Run tests
make test-fast

# See all available commands
make help
```

### CLI Usage
```bash
# Train with synthetic data (pipeline test)
python dextera.py train --synthetic --epochs 10

# Train on real data
python dextera.py train --dataset data/gestures --epochs 100 --device auto

# Evaluate
python dextera.py eval --checkpoint checkpoints/best.pt --dataset data/test

# Export to ONNX (+ quantization)
python dextera.py export --checkpoint checkpoints/best.pt --format onnx --quantize

# Webcam demo (detection-only, no trained model needed)
python dextera.py demo

# Latency benchmark
python dextera.py benchmark --checkpoint checkpoints/best.pt

# Start FastAPI server (optional â€” for remote inference)
python dextera.py serve --port 8000

# Show system info
python dextera.py info
```

### Web App
```bash
cd apps/web
npm install
npm run dev
# Open http://localhost:3000
```

## âš¡ Performance Targets

| Metric | Target |
|--------|--------|
| End-to-end latency | < 20ms |
| FPS | 60 FPS real-time |
| CPU fallback | âœ… No GPU required |
| Model size (quantized) | < 1MB |
| Memory usage | < 100MB |

## ğŸ›¡ Privacy

- **Zero cloud inference** â€” all processing on-device
- **No image/video leaves the device** â€” ever
- **Landmark-only pipeline** â€” only 21Ã—3 floats, not pixels
- GDPR / DPDP / HIPAA-safe by design

## ğŸ“œ License

MIT â€” see [LICENSE](LICENSE)
